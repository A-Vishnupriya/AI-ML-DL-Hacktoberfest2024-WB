{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Importing Necessary Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fields for processing the text and labels\n",
    "TEXT = Field(tokenize='spacy', batch_first=True)\n",
    "LABEL = Field(sequential=False, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = IMDB.splits(TEXT, LABEL)\n",
    "# Build the vocabulary\n",
    "TEXT.build_vocab(train_data, max_size=25000)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Create iterators for training and testing\n",
    "BATCH_SIZE = 64\n",
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, test_data), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Model for Sentiment Analysis:\n",
    "    This model consists of an embedding layer, LSTM layer, and a fully connected layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the embedding layer\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # Pass through the LSTM layer\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Pass the last hidden state through the fully connected layer\n",
    "        return self.fc(hidden[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(TEXT.vocab)  # Size of the vocabulary\n",
    "embedding_dim = 100  # Size of the embedding vector\n",
    "hidden_dim = 256  # Number of hidden units in LSTM\n",
    "output_dim = 1  # Output size (binary classification)\n",
    "n_layers = 2  # Number of LSTM layers\n",
    "dropout = 0.5  # Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout)\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function to train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        \n",
    "        # Get input and target sequences\n",
    "        text, labels = batch.text, batch.label\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text).squeeze(1)  # Get model predictions\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = binary_accuracy(predictions, labels)  # Calculate accuracy\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()  # Update model parameters\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Function to calculate binary accuracy.\n",
    "    \"\"\"\n",
    "    preds = torch.round(torch.sigmoid(preds))  # Round the predictions\n",
    "    correct = (preds == y).float()  # Compare predictions to labels\n",
    "    return correct.sum() / len(correct)  # Return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5  # Number of epochs\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    print(f'Epoch {epoch+1}/{N_EPOCHS} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    Function to evaluate the model on the validation or test set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch.text, batch.label\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(text).squeeze(1)  # Get model predictions\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = binary_accuracy(predictions, labels)  # Calculate accuracy\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# Cell 9: Evaluate on Test Set\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
